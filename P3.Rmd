---
title: "Untitled"
author: "Du Shi, Kunning Zhang, Yixuan Yin"
date: "2026-02-05"
output: pdf_document
---

```{r setup, echo=FALSE, message = FALSE, warning = FALSE, include = FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
source("Code.R")
set.seed(0)
```

## Executive summary

This paper will focus on classifying LLM texts and Human texts based on the difference on number of 200 function words they use in a text like 'a' and 'the', together with total other words. There will be 3 LLM models: GPT, Gemini and Llama, which will be all seen as LLM group.

We will talk about two model, 'K-nearest-neighbour' and 'Discriminant Analysis'. KNN labels points by their nearest group point. DA labels points by probability. We will introduce them later. Then we apply the chosen model DA on testing points to find a general results for future application and got an accuracy over 0.85. 

In the end, as an extra research, we will find the similarity between three LLM's texts comparing to Human. For example, we train the model by Human and GPT, and test on Gemini.

## Data preprocessing

The dataset contains around 3,500 complete English language texts presented in Stylometry form, grouped by author: Human, ChatGPT, Gemini and Llama. To solve this task, all large language model (LLM) texts were merged into a single category, resulting in a binary dataset of Human vs LLM. Then the data were randomly split into an 80% training set and a 20% testing set, and the texts were normalised to ensure comparability across texts of different lengths. Finally, the normalised 80% training set was used for exploratory data analysis.

## Exploratory Data Analysis

```{r Table 1, echo=FALSE, fig.cap="Basic Information of The Training Data", message=FALSE, warning=FALSE}
T1
```

The basic information of the training set is shown in Table 1. The training set contains 869 human written texts and 2,608 LLM written texts, with 200 function words and one additional feature for other words in each text. Then, let's look at the mean difference in function words between LLM and Human texts, where the difference is LLM minus Human. 

\newpage

```{r Figure 1, echo=FALSE, fig.cap="Top 10 Mean Difference in Function Words Between LLM and Human Texts", warning=FALSE, fig.width = 6.5, fig.height = 3}
par(mar = c(3, 4, 1, 1))
barplot(mean_diff[top], ylab = "Mean Difference")
```

The feature V201 can be ignored, as its high value is primarily due to longer text length rather than meaningful mean difference. We find that several function words have difference. In particular, V1 shows the largest mean difference, about 0.01, indicating that this function word is used very differently in human written and LLM written texts. This means V1 is an important feature for classifying human and LLM texts. Then, we used MDS visualisation to check the overall difference between Human and LLM texts.

```{r Figure 2, echo=FALSE, fig.cap="MDS Visualisation of Human and LLM Texts", warning=FALSE, fig.width = 5.5, fig.height = 3}
par(mar = c(4, 4, 1, 1))

plot(mds_res[,1], mds_res[,2],
     col = ifelse(eda_y == "Human", "blue", "red"),
     pch = 16,
     cex = 0.6,
     xlab = "MDS1",
     ylab = "MDS2")

points(human_cent[1], human_cent[2], pch = 4, cex = 2, lwd = 2, col = "blue")
points(llm_cent[1], llm_cent[2], pch = 4, cex = 2, lwd = 2, col = "Black")

legend("topleft",
       legend = c("Human texts", "LLM texts", "Human centroid", "LLM centroid"),
       col = c("blue", "red", "blue", "Black"),
       pch = c(16, 16, 4, 4),
       pt.cex = c(0.8, 0.8, 2, 2),
       bty = "n")
```

As shown in Figure 2, the Human and LLM texts overlap but have different centroids, indicating partial separation and they may be separated in later analysis. Then, we used MDS Visualisation of Authors to explore the difference between human, ChatGPT, Gemini, and Llama texts.

```{r Figure 3, echo=FALSE, fig.cap="MDS Visualisation of Authors", warning=FALSE, fig.width = 4.5, fig.height = 3.5}
plot(pts, 
     type="n", 
     xlim = c(min(pts[,1]) - 2, max(pts[,1]) + 1), 
     ylim = c(min(pts[,2]) - 1, max(pts[,2]) + 1))

text(pts[,1], pts[,2], labels=authors)
``` 

\newpage

In Figure 3, we find that LLM texts are on the right side and human texts are on the left side, indicating that human and LLM texts have different writing styles. In addition, different LLMs also have different writing styles. GPT is far away from Gemini and Llama. However, Gemini and Llama are close to each other, so they are more likely to be confused. After understanding the data, we start modelling.

## Modelling

In this section, we will discuss two models 'K-nearest neighbors' and 'Discriminant Analysis' and compare to choose a more suitable model. Based on training data, we will use LOOCV to assess each performance. Particularly for KNN, a 10-fold cross validation will be used to choose a better value 'k'.

### KNN

'K-Nearest Neighbors' (KNN) assigns a class label to a test observation based on the majority label among its k closest training samples in feature space. Distance between observations is typically measured using Euclidean distance after feature standardization. Comparing to 'DA', 'KNN' makes no distributional assumptions and relies directly on local neighborhood structure. 

However, its performance is sensitive to the choice of k, for example in case of k=1, an outlier of human data lays near LLM's group can make test points of LLM near it wrong labeled. Because of the number of LLM training points is three times of Human, a very high k is not a good choice.

From figure 4 below, 'KNN' with k from 1 to 10 all gives similar accuracy around 0.8, but the human recalls are pretty low, lead to a low balanced accuracy. It means that many human data points are wrong classified to LLM, which cause the fake high accuracy. We will discuss it later by confusion matrix in LOOCV after we choosing k=2 (k with highest human recall).

\newpage

```{r Figure 4, echo=FALSE, fig.cap="10-fold cross-validated performance of KNN showing accuracy.", warning=FALSE, fig.width = 6, fig.height = 2.5}
knn_cv_plot

```

Table 2 below shows that the LOOCV results of 'KNN' with k=2, 644 of human texts are classified to LLM, which means that the model is bad on recognizing Human texts. 

```{r echo=FALSE, warning=FALSE}
knitr::kable(cm_knn_loo$table, booktabs = TRUE, caption = "Confusion Matrix of LOOCV for KNN with k=2. ('1' is labeled as 'Human', '2' is labeled as 'LLM')")
```

Table 3 below shows that the kappa value is pretty low, so the true improvement of this model over chance is only moderate. And the sensitivity (Human recall) is much more lower than specificity (LLM recall), means that the model is biased on LLM. The reason for this may come from the imbalanced number of data points together with high overlap of two classes.

```{r echo=FALSE, warning=FALSE}
knitr::kable(knn_loo_table, digits = 3, booktabs = TRUE, caption = "KNN(k=2) Performance — LOOCV")
```

### DA

'Discriminant Analysis' assumes that observations from each class follow a multivariate normal distribution with means and variance from sample. Classification is performed by assigning each observation to the class with the higher probability. Because DA estimates a global decision boundary rather than relying on local neighbors, it is typically more stable KNN, especially when we have imbalanced number of data points. 

Based on training data, Table 4 and 5 below show that 'DA' performs much better than 'KNN' in LOOCV. Only 131 from Human and 308 from LLM are wrong classified. The accuracy is 0.874, with both high sensitivity and specificity. So 'DA' gives a fair classification with meaningful accuracy. Also kappa value (0.685) indicates substantial agreement.

```{r echo=FALSE, warning=FALSE}
knitr::kable(cm_da_cv$table,
             caption = "Confusion Matrix of LOOCV for DA. ('1' is labeled as 'Human', '2' is labeled as 'LLM')")
```

```{r echo=FALSE, warning=FALSE}
knitr::kable(da_loo_table, digits = 3,
             caption = "DA Performance — LOOCV")
```

Based on the validation results, we will choose discriminant analysis below to classify the testing data.

## Testing Results

Now we start to classify testing data by discriminant analysis. The model is trained by trainset.

From the confusion matrix below, we can see that most texts are corrected labeled.

```{r echo=FALSE, warning=FALSE}
knitr::kable(cm_da_t$table, digits = 3,
             caption = "Confusion Matrix of testing for DA. ('1' is labeled as 'Human', '2' is labeled as 'LLM')")

```

Since testset is independent with trainset and separated randomly, we have evidence to say that for any given texts, the model is able to classify them between LLM and Human at accuracy 95% confidence interval 0.841-0.888, with out obvious bias on LLM or Human.

```{r echo=FALSE, warning=FALSE}
knitr::kable(da_test_table, digits = 3, caption = "DA Performance — Testing")
```

# Extra Research

In addition to the proposal from the Bonus, we further constructed cross-model to classify the tendencies of tested LLM by training on all Human-authored texts versus one of LLM texts and testing on the other two LLM texts. To make the results more obvious, we remove the human texts in testset and only focus on if the tested LLM can be classified to trained LLM.

```{r echo=FALSE, warning=FALSE, fig.width = 6, fig.height = 2.5}
knitr::kable(summary_table, digits = 3, caption = "AI Accuracy under cross-model summary table")
```

The assessed probability of classifying the tested LLM generated texts as corresponding trained LLM varies significantly across different models. As shown in table 2, the probability of Gemini and Llama be regarded as LLM written under trained GPT and Human model are 47.4% and 61.3% respectively, indicating nearly half of the texts generated by Gemini and Llama are wrongly classified as human written, which shows a weak tendency of classifying texts. In contrast, the classifiers trained using Human and Gemini, as well as the one trained on Human and Llama, have obvious tendencies to classify the other two LLM-generated models. In particular, the proportion of correctly classified as LLM-generated texts exceeded 90% basing on both tested data from GPT and Llama.

```{r Figure 5, echo=FALSE, warning=FALSE, fig.width = 4.5, fig.height = 2.5}
ggplot(summary_table,
       aes(x = Test, y = Accuracy, group = Train, color = Train)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  labs(y = "Accuracy", title = "Accuracy if we use the “wrong” LLM to train the data") +
  theme_minimal()
```


The test results also demonstrate whether the category of LLM being tested has significant influence on classification outcomes. As shown in table 2, less than 10% of GPT-generated texts are classified as Human-written on both trained classifiers. Compared to GPT's performance, Llama and Gemini's results appeared a slightly weaker under the same classifiers. This can be easily detected from our Figure 5, which suggests that they may be a bit more challenging to distinguish from human writing. As a result, the low predictive efficiency of GPT vs Human models towards Llama and Gemini explains how, under this framework, predictive bias may be influenced simultaneously by both the classifier model itself and the type of AI being tested.

