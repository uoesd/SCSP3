---
title: "Untitled"
author: "Du Shi, Kunning Zhang, Yixuan Yin"
date: "2026-02-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source("code.R")
```

# Brief Introduction 



# Data preprocessing
The dataset contains around 3,500 complete English language text presented in Stylometry form, grouped by authors: Human, ChatGPT, Gemini and Llama. To slove this task, all large language model (LLM) were merged into a single category, resulting in a binary dataset of Human vs LLM. Then the data were randomly split into 80% training sets and 20% testing sets, and the texts were normalised to ensure comparability across texts of different lengths. Finally, The normalised 80% training set was used for exploratory data analysis.

# Exploratory Data Analysis

```{r}
# normalization(train)
normalize_rows <- function(mat) mat / rowSums(mat)
features_train_norm <- lapply(features_train, normalize_rows)

eda_X <- do.call(rbind, features_train_norm)
eda_y <- rep(c("Human", "LLM"), sapply(features_train_norm, nrow))




d1 <- data.frame("Human texts" = nrow(features_train_norm[[1]]),
                  "LLM texts" = nrow(features_train_norm[[2]]),
                  "Function words" = ncol(eda_X),
                  check.names = FALSE)
d1
```

```{r}
human_mean <- colMeans(features_train_norm[[1]])
llm_mean   <- colMeans(features_train_norm[[2]])

mean_diff <- human_mean - llm_mean
top <- order(abs(mean_diff), decreasing = TRUE)[1:10]

barplot(mean_diff[top],
        main = "Top 10 Function Words Mean Difference Between Human and LLM Texts",
        ylab = "Mean Difference")
```


```{r}
dist_mat <- dist(eda_X)
mds_res  <- cmdscale(dist_mat)

plot(mds_res[,1], mds_res[,2],
     col = ifelse(eda_y == "Human", "blue", "red"),
     pch = 16,
     cex = 0.6,
     xlab = "MDS1",
     ylab = "MDS2",
     main = "MDS Visualisation of Stylometric Distances")

human_cent <- colMeans(mds_res[eda_y == "Human", ])
llm_cent   <- colMeans(mds_res[eda_y == "LLM", ])

points(human_cent[1], human_cent[2], pch = 4, cex = 2, lwd = 2, col = "blue")
points(llm_cent[1], llm_cent[2], pch = 4, cex = 2, lwd = 2, col = "Black")

legend("topright",
       legend = c("Human texts", "LLM texts", "Human centroid", "LLM centroid"),
       col = c("blue", "red", "blue", "Black"),
       pch = c(16, 16, 4, 4),
       pt.cex = c(0.8, 0.8, 2, 2),
       bty = "n")
```

```{r}
split_idx <- lapply(features, function(mat) {
  sample(1:nrow(mat), size = floor(0.8 * nrow(mat)))
})

features_train <- mapply(function(mat, idx) {
  mat[idx, ]
}, features, split_idx, SIMPLIFY = FALSE)

features_test <- mapply(function(mat, idx) {
  mat[-idx, ]
}, features, split_idx, SIMPLIFY = FALSE)

x <- NULL
for (i in 1:length(features_train)) {
  x <- rbind(x, apply(features_train[[i]], 2, sum))
}

for (i in 1:nrow(x)) {
x[i,] <- x[i,] / sum(x[i,])
}

for (j in 1:ncol(x)) {
x[,j] <- (x[,j]- mean(x[,j]))/sd(x[,j])
}

d <- dist(x)
pts <- cmdscale(d)

plot(pts, type="n", main="MDS Plot of Authors")
text(pts[,1], pts[,2], labels=authors)

```


# Model Choices

In this section, we will discuss two models 'K-nearest neighbors' and 'Discriminant Analysis' and compare to choose a more suitable model. Based on training data, we will use LOOCV to assess each performance. Particularly for KNN, a 10-fold cross validation will be used to choose a better value 'k'.

## KNN

'K-Nearest Neighbors' (KNN) assigns a class label to a test observation based on the majority label among its k closest training samples in feature space. Distance between observations is typically measured using Euclidean distance after feature standardization. Comparing to 'DA', 'KNN' makes no distributional assumptions and relies directly on local neighborhood structure. 

However, its performance is sensitive to the choice of k, for example in case of k=1, an outlier of human data lays near AI's group can make test points of AI near it wrong labeled. Because of the number of AI training points is three times of Human, a very high k is not a good choice.

From figure [] below, 'KNN' with k from 1 to 10 all gives similar accuracy around 0.8, but the human recalls are pretty low, lead to a low balanced accuracy. It means that many human data points are wrong classified to AI, which cause the fake high accuracy. We will discuss it later in LOOCV after we choosing k=2 (k with highest human recall).

```{r}

knn_cv_plot

```

The table and confusion matrix below show the results of 'KNN' LOOCV, 658 of human texts are classified to AI, the kappa value is pretty low so the true improvement of this model over chance is only moderate. The reason for this may come from the imbalanced number of data points together with high overlap of two classes.




```{r}
knitr::kable(knn_loo_table,
             digits = 3,
             caption = "LOOCV Performance of k-NN (k = 2)")

knitr::kable(cm_knn_loo$table,
             caption = "Confusion Matrix")
```

## DA

''

```{r}
knitr::kable(da_loo_table, digits = 3,
             caption = "DA Performance — LOOCV")

```

```{r}

knitr::kable(cm_da_loo$table,
             caption = "DA (LOOCV)")
```



# Testing Results

```{r}

knitr::kable(da_test_table, digits = 3,
             caption = "DA Performance — Test Set")
```


```{r}

knitr::kable(cm_da_test$table,
             caption = "DA (Test)")

```
